# ğŸï¸ Formula 1 Data Pipeline

![Azure](https://img.shields.io/badge/Azure-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white)
![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![PySpark](https://img.shields.io/badge/Apache_Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Delta Lake](https://img.shields.io/badge/Delta_Lake-00ADD8?style=for-the-badge&logo=delta&logoColor=white)

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Architecture](#architecture)  
- [Data Source](#data-source)
- [Data Pipeline Layers](#data-pipeline-layers)
- [Tech Stack](#tech-stack)
- [Setup Instructions](#setup-instructions)
- [Features](#features)

## ğŸ¯ Overview

This project implements an end-to-end **ETL data pipeline** for Formula 1 racing data using cloud-native technologies. The pipeline extracts historical F1 data from **multiple file sources** via the Ergast Developer API, processes it using **Apache Spark on Databricks**, and stores it in a **four-layered data lake architecture** on Azure Blob Storage.

### Key Highlights
âœ… **Multi-Source Data Ingestion** from multiple file sources  
âœ… **Schema-on-Read** with External Tables in Raw zone  
âœ… **ACID-Compliant Ingestion** using Delta Lake in Bronze layer  
âœ… **Data Quality & Validation** in Silver layer  
âœ… **Analytics-Ready Datasets** in Gold layer  
âœ… **4-Layer Medallion Architecture**: Raw â†’ Bronze â†’ Silver â†’ Gold

## ğŸ—ï¸ Architecture

### High-Level Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    F1 DATA PIPELINE                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   ğŸ“¥ Data Source                
   Ergast F1 API                 
   Multiple Files                
         â”‚                       
         â”‚ Extract & Land        
         â–¼                       
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ—‚ï¸ RAW ZONE - Landing Area                                  â”‚
â”‚  â€¢ Multiple Files (JSON/CSV)                                 â”‚
â”‚  â€¢ External Hive Tables                                      â”‚
â”‚  â€¢ Schema-on-Read                                            â”‚
â”‚  â€¢ Immediate Analysis                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       
         â”‚ Read via External Tables
         â–¼                       
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âš™ï¸ AZURE DATABRICKS                                          â”‚
â”‚  PySpark Notebooks for Processing                            â”‚
â”‚  â€¢ Ingestion â†’ Transformation â†’ Aggregation                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       
         â”œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼      â–¼          â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ’¾ AZURE BLOB STORAGE (Data Lake)                            â”‚
â”‚                                                               â”‚
â”‚  ğŸ¥‰ BRONZE     â†’  ğŸ¥ˆ SILVER    â†’   ğŸ¥‡ GOLD                     â”‚
â”‚  Ingestion        Processed        Presentation              â”‚
â”‚  Delta Tables     Delta Tables     Delta Tables              â”‚
â”‚  ACID Compliant   Cleansed Data    Business Metrics          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       
         â–¼                       
   ğŸ“Š Analytics & BI             
   Power BI | Tableau            
```

### 4-Layer Medallion Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAW â†’ BRONZE â†’ SILVER â†’ GOLD                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ—‚ï¸  LAYER 1: RAW ZONE
    Purpose: Landing zone for multiple file sources
    Format: JSON/CSV files
    Tables: External Hive tables (schema-on-read)
    Benefits: 
    âœ“ Immediate data availability
    âœ“ Zero duplication  
    âœ“ Fast exploration
    âœ“ Ad-hoc SQL queries
              â†“
              
ğŸ¥‰ LAYER 2: BRONZE (Ingestion)
    Purpose: ACID-compliant ingestion
    Format: Delta Lake
    Features:
    âœ“ ACID transactions
    âœ“ Time travel
    âœ“ MERGE operations
    âœ“ Partitioned data
    âœ“ Audit columns
              â†“
              
ğŸ¥ˆ LAYER 3: SILVER (Processed)
    Purpose: Cleansed & validated data
    Format: Delta Lake  
    Transformations:
    âœ“ Data quality checks
    âœ“ Deduplication
    âœ“ Standardization
    âœ“ Type casting
    âœ“ Business rules
              â†“
              
ğŸ¥‡ LAYER 4: GOLD (Presentation)
    Purpose: Analytics-ready datasets
    Format: Delta Lake
    Content:
    âœ“ Fact tables
    âœ“ Dimension tables
    âœ“ Pre-aggregated metrics
    âœ“ Star schema
    âœ“ Business KPIs
```

## ğŸ“Š Data Source

### Ergast Developer API

**Comprehensive F1 data from 1950 to present**

- ğŸŒ **API**: [http://ergast.com/mrd/](http://ergast.com/mrd/)
- ğŸ“¦ **Dataset**: [http://ergast.com/mrd/db/](http://ergast.com/mrd/db/)
- ğŸ“… **Coverage**: 1950 - Present (70+ years)
- ğŸ“ **Format**: JSON/XML/CSV

**Available Endpoints:**
- Circuits, Races, Drivers, Constructors
- Results, Qualifying, Lap Times, Pit Stops
- Driver Standings, Constructor Standings
- Sprint Results

## ğŸ“ Data Pipeline Layers

### ğŸ—‚ï¸ LAYER 1: Raw Zone

**Purpose**: Landing area for multiple file sources

**Key Features:**
- Multiple files from Ergast API (JSON/CSV)
- External Hive tables for schema-on-read
- No data movement or duplication
- Immediate SQL query capability
- Zero processing - data as-is

**Example: Creating External Table**
```python
spark.sql("""
CREATE EXTERNAL TABLE f1_raw.circuits (
    circuitId STRING,
    circuitName STRING,
    location STRUCT<lat:STRING, long:STRING, 
                    locality:STRING, country:STRING>,
    url STRING
) USING JSON
LOCATION 'abfss://raw@storage.dfs.core.windows.net/circuits/'
""")

# Query immediately without data movement
spark.sql("SELECT * FROM f1_raw.circuits").show()
```

---

### ğŸ¥‰ LAYER 2: Bronze (Ingestion)

**Purpose**: ACID-compliant ingestion with Delta Lake

**Key Features:**
- Delta Lake format with ACID properties
- Data read from Raw zone external tables
- MERGE operations for idempotency
- Partitioned by year/race
- Time travel enabled
- Audit columns (ingestion_date, source_file)

**Example: Bronze Ingestion**
```python
from delta.tables import DeltaTable
from pyspark.sql.functions import current_timestamp, lit

# Read from Raw external table
df_raw = spark.sql("SELECT * FROM f1_raw.circuits")

# Add audit columns
df_bronze = df_raw \
    .withColumn("ingestion_date", current_timestamp()) \
    .withColumn("source", lit("ergast_api"))

# Write as Delta with MERGE
bronze_path = "abfss://bronze@storage.dfs.core.windows.net/circuits_bronze"

if DeltaTable.isDeltaTable(spark, bronze_path):
    deltaTable = DeltaTable.forPath(spark, bronze_path)
    deltaTable.alias("target").merge(
        df_bronze.alias("source"),
        "target.circuitId = source.circuitId"
    ).whenMatchedUpdateAll() \
     .whenNotMatchedInsertAll() \
     .execute()
else:
    df_bronze.write.format("delta").save(bronze_path)

# Optimize for performance
spark.sql(f"OPTIMIZE delta.`{bronze_path}`")
```

---

### ğŸ¥ˆ LAYER 3: Silver (Processed)

**Purpose**: Cleansed, validated, standardized data

**Key Features:**
- Delta Lake format
- Data quality checks applied
- Deduplication
- NULL handling
- Type casting
- Standardized column names (snake_case)
- Business rule validation

**Example: Silver Transformation**
```python
from pyspark.sql.functions import col, trim, upper, coalesce

# Read from Bronze
df_bronze = spark.read.format("delta").load(bronze_path)

# Apply transformations
df_silver = (df_bronze
    .dropDuplicates(["circuitId"])
    .withColumnRenamed("circuitId", "circuit_id")
    .withColumnRenamed("circuitName", "circuit_name")
    .withColumn("circuit_name", trim(col("circuit_name")))
    .withColumn("country", upper(col("location.country")))
    .withColumn("latitude", col("location.lat").cast("double"))
    .withColumn("longitude", col("location.long").cast("double"))
    .withColumn("locality", coalesce(col("location.locality"), 
                                     lit("Unknown")))
    .filter(col("circuit_id").isNotNull())
    .drop("location", "source")
)

# Write to Silver
silver_path = "abfss://silver@storage.dfs.core.windows.net/circuits_processed"
df_silver.write.format("delta").mode("overwrite").save(silver_path)
```

---

### ğŸ¥‡ LAYER 4: Gold (Presentation)

**Purpose**: Analytics-ready business datasets

**Key Features:**
- Delta Lake format
- Star/Snowflake schema
- Fact & dimension tables
- Pre-calculated metrics
- Denormalized for performance
- Optimized for BI tools

**Example: Gold Aggregation**
```python
from pyspark.sql.functions import sum, avg, count, when, dense_rank
from pyspark.sql.window import Window

# Read from Silver
df_results = spark.read.format("delta").load(results_silver_path)
df_drivers = spark.read.format("delta").load(drivers_silver_path)

# Create driver performance summary
df_gold = (df_results.join(df_drivers, "driver_id")
    .groupBy("driver_id", "driver_name", "season")
    .agg(
        count("*").alias("races_entered"),
        sum(when(col("final_position") == 1, 1).otherwise(0)).alias("wins"),
        sum(when(col("final_position") <= 3, 1).otherwise(0)).alias("podiums"),
        sum("points_scored").alias("total_points"),
        avg("final_position").alias("avg_finish_position")
    )
    .withColumn("win_rate", col("wins") / col("races_entered") * 100)
    .withColumn("podium_rate", col("podiums") / col("races_entered") * 100)
)

# Add championship ranking
window_spec = Window.partitionBy("season").orderBy(col("total_points").desc())
df_gold = df_gold.withColumn("championship_position", dense_rank().over(window_spec))

# Write to Gold
gold_path = "abfss://gold@storage.dfs.core.windows.net/driver_performance"
df_gold.write.format("delta").partitionBy("season").save(gold_path)
```

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Cloud** | Microsoft Azure |
| **Storage** | Azure Blob Storage (ADLS Gen2) |
| **Processing** | Apache Spark (PySpark) |
| **Compute** | Azure Databricks |
| **Format** | Delta Lake |
| **External Tables** | Hive Metastore |
| **Language** | Python 3.8+ |

**Why Delta Lake?**
- âœ… ACID transactions
- âœ… Time travel (versioning)
- âœ… Schema enforcement & evolution
- âœ… MERGE/UPDATE/DELETE operations
- âœ… Performance optimization (OPTIMIZE, Z-ORDER)

## ğŸš€ Setup Instructions

### Prerequisites
- Azure subscription
- Azure Databricks workspace
- Azure Storage Account (ADLS Gen2)

### Quick Setup

```bash
# 1. Create Storage Account
az storage account create \
  --name f1datalake001 \
  --resource-group f1-rg \
  --location eastus \
  --sku Standard_LRS \
  --hierarchical-namespace true

# 2. Create containers
az storage container create --name raw --account-name f1datalake001
az storage container create --name bronze --account-name f1datalake001  
az storage container create --name silver --account-name f1datalake001
az storage container create --name gold --account-name f1datalake001

# 3. Create Databricks workspace
az databricks workspace create \
  --resource-group f1-rg \
  --name f1-databricks \
  --location eastus \
  --sku premium
```

### Mount Storage in Databricks

```python
# Mount all layers
storage_account = "f1datalake001"
key = "<storage-key>"

configs = {f"fs.azure.account.key.{storage_account}.dfs.core.windows.net": key}

for layer in ["raw", "bronze", "silver", "gold"]:
    dbutils.fs.mount(
        source=f"abfss://{layer}@{storage_account}.dfs.core.windows.net/",
        mount_point=f"/mnt/f1dl/{layer}",
        extra_configs=configs
    )
```

## âœ¨ Features

### Implemented
âœ… 4-Layer Medallion Architecture  
âœ… External Tables in Raw Zone  
âœ… Delta Lake Across All Layers  
âœ… ACID Transactions  
âœ… Time Travel Capability  
âœ… Data Quality Framework  
âœ… Incremental Loading (MERGE)  
âœ… Partitioning Strategy  
âœ… Performance Optimization  
âœ… 70+ Years of F1 Data  

### Future Enhancements
ğŸ”® Real-time Streaming  
ğŸ”® Machine Learning Models  
ğŸ”® Power BI Dashboards  
ğŸ”® CI/CD Pipeline  
ğŸ”® Advanced Analytics  
ğŸ”® Data Catalog (Unity Catalog)  

## ğŸ‘¤ Author

**Vrukkodhara**
- GitHub: [@vrukkodhara](https://github.com/vrukkodhara)

## ğŸ“„ License

MIT License

---

**Made with â¤ï¸ for F1 enthusiasts and data engineers**

ğŸï¸ *"Every lap counts, every layer matters"* ğŸ
